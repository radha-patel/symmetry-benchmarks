# Systec Artifact

This artifact provides all of the necessary tools to reproduce the program
compilation and charts presented in the paper, with the exception of SSYRK
(which takes too much time and memory), and MKL, which is too cumbersome to
install. The datasets given to TTM and MTTKRP are smaller here than in the
paper. We claim that all speedups should be within 1.5 of the expected, with few
exceptions.  We note that there was a bug in the TACO TTM results in the
submitted version which has been fixed here, so the results may not match the
submitted version, but do match the camera ready.

## Artifact check-list (meta-information)

- Algorithm:  We present a new compilation algorithm, SySTeC, which accepts pointwise einsums with symmetry annotated-tensors and produces symmetry-optimized Finch tensor programs.
- Program:  We benchmark on the SSYMV, SYPRD, TTM, and MTTKRP
- Kernels. We compare against naive Finch, TACO, and SPLATT, all of which are
included.
- Compilation:  We require Finch v0.6.32 and Julia v1.10.4 to compile.
- Binary:  We provide a Docker image \texttt{systec-artifact.tar} with all dependencies pre-installed, which requires ARM hardware to run as it was built for Apple M2. If different hardware is used, one can build the docker image for that hardware using the provided instructions.
- Data set:  Matrix datasets are downloaded automatically from \url{http://sparse.tamu.edu/}.
- Run-time environment:  The artifact requires a Unix System (Mac or Linux).
- Hardware:  We benchmark on an Intel Xeon CPU E5-2680 v3 running at 2.50GHz, and our benchmarks run well on Apple M2.
- Execution:  We assume that TACO and SPLATT will run in single-threaded mode.
- Metrics:  We measure runtime of all methods we test.
- Output:  The numerical output of each kernel is collected and validated against comparison methods in the script itself. The experiments themselves output json files in the same subdirectory they run in. The artifact includes instructions to plot the results.
- Experiments:  We claim that our speedup results should be within 1.5 of the expected, with few exceptions.
- How much disk space required (approximately)?:  4GB
- How much time is needed to prepare workflow (approximately)?:  The artifact can be set up quickly with Docker, in around an hour. Building by hand also takes less than an hour, but it may take longer if your system needs special build flags.
- How much time is needed to complete experiments (approximately)?:  The experiments take around an hour to run.
- Publicly available?:  yes
- Code licenses (if publicly available)?:  MIT
- Workflow framework used?:  Docker

# Description

## Hardware dependencies

These experiments require at least 24GB of memory and 4GB disk space. A network
connection is required to download the matrix datasets.

We benchmark on an Intel Xeon CPU E5-2680 v3 running at 2.50GHz, and our benchmarks run well on Apple M2.

## Software dependencies

A Unix (Mac or Linux) system is required to run the artifact. ARM hardware is required to use the pre-built docker image included with the artifact.

Dependencies which must be downloaded are documented in \texttt{Dockerfile}:
- Julia v1.10.4
- coreutils
- cmake
- gcc
- g++
- python
- python3
- python3-pip
- python3-venv
- git
- libblas-dev
- liblapack-dev
- poetry

Dependencies which are built from source with \texttt{make deps}:
- [TACO](https://github.com/tensor-compiler/taco/commit/1278503a1c859d557174a4ef2ae7a85295f39f69)
- [SPLATT](https://github.com/ShadenSmith/splatt/commit/6cb86283c1fbfddcc67c2564e025691de4f784cf)

Julia deps are also collected in a Project.toml file which can be installed with `julia setup.jl` or just `make env`.
-  ArgParse v1.2.0
-  BenchmarkTools v1.5.0
-  DataStructures v0.18.20
-  Finch v0.6.32
-  JSON v0.21.4
-  MatrixDepot v1.0.13
-  SparseArrays v1.10.0
-  SySTeC v0.1.0 \url{https://github.com/radha-patel/SySTeC/commit/b0ec98927f0d2be01a61b48646cbadaa92040b0f}
-  TensorMarket v0.2.0

Python deps are collected in a pyproject.toml file which can be installed with `poetry install --no-root` or just `make env`.
-  python = v3.9
-  matplotlib = v3.9.0
-  packaging = v24.0
-  numpy = v1.26.4

## Data sets

All matrix datasets are automatically downloaded with a network connection from `http://sparse.tamu.edu/`

The tensor datasets are randomly generated by the benchmarks themselves.

# Installation

## Docker
Installation is greatly simplified if you are using the provided Docker image.
First load the image with
```
docker image load -i systec-artifact.tar
```
or build it with
```
docker build -t systec-artifact .
```

Then, you can launch a shell in the container named "evaluator-container" with
```
docker run -it --name evaluator systec-artifact /bin/bash
```

Be sure to configure Docker to use enough memory (24GB) and disk space (100 GB) for the experiments.

## Manual
Once the basic dependencies are installed on your system, you should be able to run `make` to build all the necessary dependencies.

Should you need to customize the workflow, `make deps` builds the
dependencies, `make env` initializes Python and Julia Environments,
and `make kernels` builds the benchmarking kernels.

# Experiment workflow

There are three main phases to the experiments:

## Run SySTeC to compile the kernels

From the toplevel directory, you can run the SySTeC compiler with

```
julia run_SySTeC.jl
```

This will compile all the kernels required for the experiments and output them into the `generated/` directory.
The script itself can be inspected and modified if you wish to compile different kernels and compare the output code.
The SySTeC compiler itself is contained in `deps/SySTeC/src/SySTeC.jl`.

## Run the experiements

You can run the experiments with
```
sh run_benchmarks.sh
```
Benchmarks took us a little over an hour to run. The benchmarks may be run
individually with the individual commands in the `run_benchmarks.sh` script.
Results will be generated in the toplevel directory. Corresponding reference
results used in the publication are available with the filename suffix
`_reference.json`.

## Plot the results

You can plot the results with
```
poetry run python plot_results.py
```

This will generate
plots for whichever experiments you have run and saved results for. The plots
will be saved in the `charts/` directory. Corresponding reference results used in the
publication are available with the filename suffix `_reference.json`.

# Evaluation and expected result

You should expect to see similar results to the paper, notably SSYMV (Figure 5), SYPRD (Figure 6), TTM (Figure 7), and MTTKRP (Figure 8).  We expect
that the more dramatic speedups will be seen in the MTTKRP benchmarks, as the
compiler is able to exploit the most symmetry in 5-dimensional kernels. Note
that we have reduced the size of the tensors in TTM and MTTKRP to keep the
runtime and storage manageable, so these results may demonstrate slightly less
speedup as more time is spent on diagonal edge cases.

If you are using our provided Docker image or using Docker to run the
experiments, you can view the charts by copying them to the host.
```
docker cp evaluator:symmetry-benchmarks/charts .
```

# Experiment customization

The evaluator may optionally customize the kernel given to SySTeC by
modifying the `run_SySTeC.jl` script. The size of the tensors in our TTM
and MTTKRP experiments can be changed by modifying the scripts directly.

# Methodology

Submission, reviewing and badging methodology:

  -  [`http://cTuning.org/ae/submission-20190109.html`](`http://cTuning.org/ae/submission-20190109.html)
  -  [`http://cTuning.org/ae/reviewing-20190109.html`](http://cTuning.org/ae/reviewing-20190109.html)
  -  [`https://www.acm.org/publications/policies/artifact-review-badging`](https://www.acm.org/publications/policies/artifact-review-badging)
